{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: []\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import AUROC\n",
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram, Resample\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    LlamaConfig, \n",
    "    LlamaModel, \n",
    "    LlamaTokenizer, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoModel\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    IA3Config, \n",
    "    get_peft_model, \n",
    "    TaskType\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPERA_CT_TARGET_MODULES = [\"qkv\", \"proj\"]\n",
    "OPERA_CE_TARGET_MODULES = [\"conv\", \"fc\", \"linear\"]\n",
    "target_module_dict = {\n",
    "    \"operaCT\": OPERA_CT_TARGET_MODULES, \n",
    "    \"operaCE\": OPERA_CE_TARGET_MODULES\n",
    "}\n",
    "\n",
    "LLM_TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
    "LLM_TARGET_MODULES_ALLPROJ = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_to_spectrogram(\n",
    "    audio_file_path: str,\n",
    "    target_sr: int = 16000,\n",
    "    n_fft: int = 400,\n",
    "    hop_length: int = 160,\n",
    "    n_mels: int = 80\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    waveform, sr = torchaudio.load(audio_file_path)\n",
    "    # Nếu waveform có nhiều kênh (stereo), ta lấy trung bình để được 1 kênh\n",
    "    if waveform.shape[0] > 1:\n",
    "        waveform = waveform.mean(dim=0, keepdim=True)\n",
    "\n",
    "    if sr != target_sr:\n",
    "        resampler = Resample(sr, target_sr)\n",
    "        waveform = resampler(waveform)\n",
    "\n",
    "    mel_transform = MelSpectrogram(\n",
    "        sample_rate=target_sr,\n",
    "        n_fft=n_fft,\n",
    "        hop_length=hop_length,\n",
    "        n_mels=n_mels\n",
    "    )\n",
    "    mel_spec = mel_transform(waveform)  \n",
    "\n",
    "    return mel_spec\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OperaCTEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=256, out_dim=256):\n",
    "        super().__init__()\n",
    "        self.qkv = nn.Conv1d(in_dim, out_dim, kernel_size=3, padding=1)\n",
    "        self.proj = nn.Linear(out_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (B, in_dim, time)\n",
    "        x = self.qkv(x)  # (B, out_dim, time)\n",
    "        x = F.relu(x)\n",
    "        x = x.mean(dim=-1)  # (B, out_dim)\n",
    "        x = self.proj(x)    # (B, out_dim)\n",
    "        return x\n",
    "\n",
    "    def forward_window(self, x):\n",
    "        return self.forward(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, nf, out_dim, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, out_dim)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x, no_fc=False):\n",
    "        # x shape: (B, d_ff, patch_nums)\n",
    "        x = self.flatten(x)  # (B, d_ff * patch_nums)\n",
    "        if no_fc:\n",
    "            return x\n",
    "        x = self.linear(x)   # (B, out_dim)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RespLLM(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(RespLLM, self).__init__()\n",
    "\n",
    "        self.n_cls = configs.n_cls\n",
    "        self.d_ff = configs.d_ff\n",
    "        self.d_llm = configs.llm_dim\n",
    "        self.audio_peft = configs.audio_peft\n",
    "        self.d_audio = configs.enc_dim\n",
    "        self.patch_nums = configs.patch_nums\n",
    "\n",
    "        self.llm_peft = configs.llm_peft\n",
    "        self.llm_lora_rank = configs.llm_lora_rank\n",
    "        self.llm_lora_alpha = configs.llm_lora_alpha\n",
    "        self.llm_lora_dropout = configs.llm_lora_dropout\n",
    "\n",
    "        self.use_audio = configs.use_audio\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "        if configs.llm_model == 'llama3':\n",
    "            self.llama_config = LlamaConfig.from_pretrained('meta-llama/Llama-3.1-8B')\n",
    "            try:\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    'meta-llama/Llama-3.1-8B',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.llama_config,\n",
    "                )\n",
    "            except EnvironmentError:\n",
    "                print(\"Không tìm thấy model cục bộ, thử tải về ...\")\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    'meta-llama/Llama-3.1-8B',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.llama_config,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    'meta-llama/Llama-3.1-8B',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "            except EnvironmentError:\n",
    "                print(\"Không tìm thấy tokenizer cục bộ, thử tải về ...\")\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    'meta-llama/Llama-3.1-8B',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "\n",
    "        self.base_audio_encoder = OperaCTEncoder(\n",
    "            in_dim=self.d_audio, \n",
    "            out_dim=self.d_audio\n",
    "        )\n",
    "\n",
    "        if self.audio_peft == \"lora\":\n",
    "            peft_config = LoraConfig(\n",
    "                r=configs.audio_lora_rank,\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=target_module_dict[configs.audio_encoder]\n",
    "            )\n",
    "        elif self.audio_peft == \"IA3\":\n",
    "            peft_config = IA3Config(\n",
    "                target_modules=target_module_dict[configs.audio_encoder],\n",
    "                feedforward_modules=['proj']\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"Audio fine-tuning mode undefined\")\n",
    "\n",
    "        self.audio_encoder = get_peft_model(self.base_audio_encoder, peft_config)\n",
    "        self.audio_encoder.print_trainable_parameters()\n",
    "\n",
    "        if configs.aligner == \"projection\":\n",
    "            self.aligner = nn.Linear(self.d_audio, self.d_llm)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Aligner module undefined\")\n",
    "\n",
    "        self.head_dropout = configs.head_dropout\n",
    "        self.output_projection = FlattenHead(self.d_ff * self.patch_nums, self.n_cls, head_dropout=self.head_dropout)\n",
    "\n",
    "        self.print_trainable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinitialize_clf(self, n_cls):\n",
    "        self.output_projection = FlattenHead(self.d_ff * self.patch_nums, n_cls, head_dropout=self.head_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable(self):\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        print(\"total trainable parameters:\", trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_trainable(self):\n",
    "        if self.llm_peft == \"frozen\":\n",
    "            for param in self.llm_model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        if self.audio_peft == \"frozen\":\n",
    "            for param in self.audio_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif self.audio_peft == \"full\":\n",
    "            for param in self.audio_encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        for param in self.aligner.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        for param in self.output_projection.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.print_trainable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x_spectrogram, x_prompt, x_context, no_fc=False):\n",
    "        # Encode audio \n",
    "        if self.patch_nums == 1:\n",
    "            x_enc = self.audio_encoder(x_spectrogram)  # (B, d_audio)\n",
    "            enc_out = self.aligner(x_enc)              # (B, d_llm)\n",
    "            enc_out = enc_out.unsqueeze(dim=1)         # (B, 1, d_llm)\n",
    "        elif self.patch_nums == 64:\n",
    "            x_enc = self.audio_encoder.forward_window(x_spectrogram)\n",
    "            enc_out = self.aligner(x_enc)  # Giả sử (B, 64, d_llm)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        prompt = self.tokenizer(\n",
    "            x_prompt, return_tensors=\"pt\", \n",
    "            padding=True, truncation=True, max_length=2048\n",
    "        ).input_ids.to(x_enc.device)\n",
    "\n",
    "        context = self.tokenizer(\n",
    "            x_context, return_tensors=\"pt\", \n",
    "            padding=True, truncation=True, max_length=2048\n",
    "        ).input_ids.to(x_enc.device)\n",
    "\n",
    "        prompt_embeddings = self.llm_model.get_input_embeddings()(prompt)     # (B, prompt_len, d_llm)\n",
    "        context_embeddings = self.llm_model.get_input_embeddings()(context)   # (B, context_len, d_llm)\n",
    "\n",
    "        if self.use_audio:\n",
    "            llama_enc_out = torch.cat([prompt_embeddings, context_embeddings, enc_out], dim=1)\n",
    "        else:\n",
    "            llama_enc_out = torch.cat([prompt_embeddings, context_embeddings], dim=1)\n",
    "\n",
    "        dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state\n",
    "        # dec_out shape: (B, seq_len, d_llm)\n",
    "\n",
    "        # Giả sử ta chỉ giữ lại d_ff đầu (nếu d_llm >= d_ff)\n",
    "        dec_out = dec_out[:, :, :self.d_ff]  # (B, seq_len, d_ff)\n",
    "        dec_out = dec_out.permute(0, 2, 1).contiguous()  # (B, d_ff, seq_len)\n",
    "\n",
    "        # Lấy patch_nums cuối (tuỳ ý) rồi cho qua head \n",
    "        dec_out = dec_out[:, :, -self.patch_nums:]  # (B, d_ff, patch_nums)\n",
    "        logits = self.output_projection(dec_out, no_fc=no_fc)  # (B, n_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    n_cls = 10\n",
    "    d_ff = 512\n",
    "    llm_dim = 1024\n",
    "    audio_peft = \"lora\"\n",
    "    enc_dim = 256\n",
    "    patch_nums = 1\n",
    "    llm_peft = \"frozen\"     # hoặc \"lora\" nếu muốn LoRA cho LLM\n",
    "    llm_lora_rank = 8\n",
    "    llm_lora_alpha = 32\n",
    "    llm_lora_dropout = 0.1\n",
    "    use_audio = True\n",
    "    llm_model = \"llama3\"    # giả sử 'llama3' tương ứng Llama-3.1 8B\n",
    "    audio_lora_rank = 4\n",
    "    audio_encoder = \"operaCT\"\n",
    "    aligner = \"projection\"\n",
    "    head_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaModel\n",
    "\n",
    "model = LlamaModel.from_pretrained(\n",
    "  \"meta-llama/Llama-3.1-8B\",\n",
    "  use_auth_token=\"[Hugging Face API Here]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    configs = Config()\n",
    "    model = RespLLM(configs)\n",
    "    spectrogram = load_audio_to_spectrogram(\"example.wav\")  \n",
    " \n",
    "    model.d_audio = 80\n",
    "    model.audio_encoder.base_model.qkv.in_channels = 80\n",
    "    # (Cách làm triệt để là khởi tạo model lại với config.enc_dim=80)\n",
    "    \n",
    "    # Reshape spectrogram -> (B=1, in_dim=80, time=?)\n",
    "    spectrogram = spectrogram.to(torch.float32)\n",
    "    B, n_mels, T = spectrogram.shape  # B=1, n_mels=80, T=? \n",
    "    spectrogram = spectrogram.view(1, n_mels, T)\n",
    "\n",
    "    x_prompt = \"Đây là một prompt thử nghiệm.\"\n",
    "    x_context = \"Đây là phần context thử nghiệm.\"\n",
    "\n",
    "    output = model.forward(spectrogram, x_prompt, x_context)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output:\", output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
