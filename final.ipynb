{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "919b49dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa, librosa.display\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89128e",
   "metadata": {},
   "source": [
    "## 2. Paths & Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ae07f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Paths (refactored for new folder layout) ──────────────────────────────────\n",
    "import os\n",
    "\n",
    "# 1. Base data directory (the folder that contains both your raw download and metadata.csv)\n",
    "DATA_ROOT       = \"data\"\n",
    "\n",
    "# 2. Raw audio + annotation files are in the downloaded ICBHI folder\n",
    "RAW_DIR         = os.path.join(DATA_ROOT, \"ICBHI_final_database\")\n",
    "\n",
    "# 3. Since .wav and .txt both live here, point both to RAW_DIR\n",
    "AUDIO_DIR       = RAW_DIR\n",
    "ANNOTATION_DIR  = RAW_DIR\n",
    "\n",
    "# 4. Metadata CSV lives alongside the download folder\n",
    "METADATA_FILE   = os.path.join(DATA_ROOT, \"metadata.csv\")\n",
    "\n",
    "# 5. Cache directory for spectrogram images or pre-computed features\n",
    "CACHE_DIR       = os.path.join(DATA_ROOT, \"cache_imgs\")\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Sanity checks\n",
    "assert os.path.isdir(RAW_DIR),        f\"❌ RAW_DIR not found: {RAW_DIR}\"\n",
    "assert os.path.isfile(METADATA_FILE), f\"❌ metadata.csv not found: {METADATA_FILE}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89caecb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audio parameters\n",
    "SR         = 4000    # common resample rate\n",
    "WIN_SEC    = 3\n",
    "HOP_SEC    = 3\n",
    "\n",
    "# STFT params\n",
    "N_FFT      = 512\n",
    "HOP_LENGTH = 256\n",
    "\n",
    "# Image size\n",
    "IMG_SIZE   = 128\n",
    "\n",
    "# Training params\n",
    "BATCH_SIZE = 32\n",
    "LR         = 1e-4\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EPOCHS     = 30\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d985e106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Successfully created a complete metadata table ---\n",
      "Found 920 recordings with matching diagnoses.\n",
      "\n",
      "--- Dataset Overview ---\n",
      "Total recordings: 920\n",
      "\n",
      "Recordings per split:\n",
      "split\n",
      "train    706\n",
      "test     214\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Patients per split:\n",
      "split\n",
      "test      26\n",
      "train    100\n",
      "Name: patient_id, dtype: int64\n",
      "\n",
      "Class distribution (overall):\n",
      "diagnosis\n",
      "COPD              793\n",
      "Pneumonia          37\n",
      "Healthy            35\n",
      "URTI               23\n",
      "Bronchiectasis     16\n",
      "Bronchiolitis      13\n",
      "LRTI                2\n",
      "Asthma              1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Classes: ['Asthma', 'Bronchiectasis', 'Bronchiolitis', 'COPD', 'Healthy', 'LRTI', 'Pneumonia', 'URTI']\n"
     ]
    }
   ],
   "source": [
    "# Load the provided metadata which contains diagnosis info\n",
    "df_diag = pd.read_csv('metadata.csv')\n",
    "# The 'id' column corresponds to the patient ID\n",
    "df_diag = df_diag.rename(columns={'id': 'patient_id'})\n",
    "\n",
    "# --- Step 1: Discover audio files and create a full metadata table ---\n",
    "# Scan the audio directory to find all .wav files\n",
    "audio_files = glob.glob(os.path.join(AUDIO_DIR, \"*.wav\"))\n",
    "\n",
    "if not audio_files:\n",
    "    raise FileNotFoundError(f\"No .wav files found in {AUDIO_DIR}. Please ensure your audio files are in the correct directory.\")\n",
    "\n",
    "# Create a list of metadata from the filenames found\n",
    "file_metadata = []\n",
    "for fpath in audio_files:\n",
    "    fname = os.path.basename(fpath)\n",
    "    # Extract patient ID from filename like '101_1b1_Al_sc_Meditron.wav'\n",
    "    patient_id = int(fname.split('_')[0])\n",
    "    file_metadata.append({\n",
    "        'patient_id': patient_id,\n",
    "        'filename': fname\n",
    "    })\n",
    "df_files = pd.DataFrame(file_metadata)\n",
    "\n",
    "# --- Step 2: Merge file info with diagnosis info ---\n",
    "# This links filenames to their diagnosis via the patient_id\n",
    "df_meta = pd.merge(df_files, df_diag, on='patient_id', how='left')\n",
    "\n",
    "# Drop any files that didn't have a matching diagnosis\n",
    "df_meta = df_meta.dropna(subset=['diagnosis'])\n",
    "\n",
    "print(\"--- Successfully created a complete metadata table ---\")\n",
    "print(f\"Found {len(df_meta)} recordings with matching diagnoses.\")\n",
    "\n",
    "\n",
    "# --- Step 3: Perform a Patient-Aware Train-Test Split ---\n",
    "# We split based on unique patient IDs to prevent data leakage\n",
    "splitter = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "groups = df_meta['patient_id']\n",
    "train_idx, test_idx = next(splitter.split(df_meta, groups=groups))\n",
    "\n",
    "# Add the 'split' column to our new metadata table\n",
    "df_meta['split'] = 'train'\n",
    "df_meta.loc[test_idx, 'split'] = 'test'\n",
    "\n",
    "\n",
    "# --- Step 4: Display Dataset Overview ---\n",
    "print(\"\\n--- Dataset Overview ---\")\n",
    "print(\"Total recordings:\", len(df_meta))\n",
    "\n",
    "print(\"\\nRecordings per split:\")\n",
    "print(df_meta['split'].value_counts())\n",
    "\n",
    "print(\"\\nPatients per split:\")\n",
    "print(df_meta.groupby('split')['patient_id'].nunique())\n",
    "\n",
    "print(\"\\nClass distribution (overall):\")\n",
    "print(df_meta['diagnosis'].value_counts())\n",
    "\n",
    "# Map string labels to integers (as in original code)\n",
    "labels = sorted(df_meta['diagnosis'].unique())\n",
    "label2idx = {lbl: i for i, lbl in enumerate(labels)}\n",
    "print(\"\\nClasses:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "196c1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_audio(y, sr, win_sec=3, hop_sec=3):\n",
    "    win_len = int(win_sec * sr)\n",
    "    hop_len = int(hop_sec * sr)\n",
    "    segments = []\n",
    "    # The last segment is allowed to be shorter than win_len\n",
    "    for start in range(0, len(y), hop_len):\n",
    "        end = start + win_len\n",
    "        segments.append(y[start:end])\n",
    "    # If the last segment is shorter than win_len, pad it\n",
    "    if len(segments[-1]) < win_len:\n",
    "        segments[-1] = np.pad(segments[-1], (0, win_len - len(segments[-1])), 'constant')\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecdaa9f3",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'row' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m df_test_meta  \u001b[38;5;241m=\u001b[39m df_meta[df_meta\u001b[38;5;241m.\u001b[39msplit \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Create manifests for training and testing sets\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m df_train \u001b[38;5;241m=\u001b[39m create_segment_manifest(df_train_meta, AUDIO_DIR, SR, WIN_SEC, HOP_SEC)\n\u001b[1;32m     36\u001b[0m df_test  \u001b[38;5;241m=\u001b[39m create_segment_manifest(df_test_meta,  AUDIO_DIR, SR, WIN_SEC, HOP_SEC)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTotal training files: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_train_meta)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> Total training segments: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_train)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m, in \u001b[0;36mcreate_segment_manifest\u001b[0;34m(df, audio_dir, sr, win_sec, hop_sec)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_segment_manifest\u001b[39m(df, audio_dir, sr, win_sec, hop_sec):\n\u001b[1;32m      2\u001b[0m     manifest \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow\u001b[38;5;241m.\u001b[39msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m manifest\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m      4\u001b[0m         fname \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mfilename\n\u001b[1;32m      5\u001b[0m         label \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mdiagnosis\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'row' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "def create_segment_manifest(df, audio_dir, sr, win_sec, hop_sec):\n",
    "    manifest = []\n",
    "    for _, row in tqdm(df.iterrows(), total=df.shape[0], desc=f\"Creating {row.split} manifest\"):\n",
    "        fname = row.filename\n",
    "        label = row.diagnosis\n",
    "        audio_path = os.path.join(audio_dir, fname)\n",
    "        \n",
    "        try:\n",
    "            y, _ = librosa.load(audio_path, sr=sr)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load {fname}: {e}\")\n",
    "            continue\n",
    "\n",
    "        win_len = int(win_sec * sr)\n",
    "        hop_len = int(hop_sec * sr)\n",
    "        \n",
    "        num_segments = (len(y) - win_len) // hop_len + 1\n",
    "        if num_segments <= 0:\n",
    "            num_segments = 1\n",
    "        \n",
    "        for i in range(num_segments):\n",
    "            manifest.append({\n",
    "                \"filename\": fname,\n",
    "                \"segment_idx\": i,\n",
    "                \"diagnosis\": label\n",
    "            })\n",
    "            \n",
    "    return pd.DataFrame(manifest)\n",
    "\n",
    "# Split the master metadata table\n",
    "df_train_meta = df_meta[df_meta.split == 'train']\n",
    "df_test_meta  = df_meta[df_meta.split == 'test']\n",
    "\n",
    "# Create manifests for training and testing sets\n",
    "df_train = create_segment_manifest(df_train_meta, AUDIO_DIR, SR, WIN_SEC, HOP_SEC)\n",
    "df_test  = create_segment_manifest(df_test_meta,  AUDIO_DIR, SR, WIN_SEC, HOP_SEC)\n",
    "\n",
    "print(f\"\\nTotal training files: {len(df_train_meta)} -> Total training segments: {len(df_train)}\")\n",
    "print(f\"Total testing files: {len(df_test_meta)} -> Total testing segments: {len(df_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e211133d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spectrogram(y, sr):\n",
    "    S = np.abs(librosa.stft(y, n_fft=N_FFT, hop_length=HOP_LENGTH))\n",
    "    S = librosa.util.normalize(S)\n",
    "    S = cv2.resize(S, (IMG_SIZE, IMG_SIZE))\n",
    "    return (S * 255).astype(np.uint8)\n",
    "\n",
    "def make_mfcc(y, sr, n_mfcc=13):\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc, n_fft=N_FFT, hop_length=HOP_LENGTH)\n",
    "    mfcc = librosa.util.normalize(mfcc)\n",
    "    mfcc = cv2.resize(mfcc, (IMG_SIZE, IMG_SIZE))\n",
    "    return (mfcc * 255).astype(np.uint8)\n",
    "\n",
    "def make_chroma(y, sr):\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr, hop_length=HOP_LENGTH)\n",
    "    chroma = librosa.util.normalize(chroma)\n",
    "    chroma = cv2.resize(chroma, (IMG_SIZE, IMG_SIZE))\n",
    "    return (chroma * 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "13e9255a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 47\u001b[0m\n\u001b[1;32m     44\u001b[0m transform \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mNormalize(mean\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.5\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m, std\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0.2\u001b[39m]\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Create Datasets using the new segment manifests\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m train_ds \u001b[38;5;241m=\u001b[39m ICBHIDataset(df_train, AUDIO_DIR, CACHE_DIR, transform)\n\u001b[1;32m     48\u001b[0m test_ds  \u001b[38;5;241m=\u001b[39m ICBHIDataset(df_test,  AUDIO_DIR, CACHE_DIR, transform)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Create DataLoaders\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "class ICBHIDataset(Dataset):\n",
    "    def __init__(self, df, audio_dir, cache_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.cache_dir = cache_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.loc[idx]\n",
    "        fname = row.filename\n",
    "        seg_idx = row.segment_idx\n",
    "        label = label2idx[row.diagnosis]\n",
    "        \n",
    "        # Cache path is now unique for each segment\n",
    "        base = os.path.splitext(fname)[0]\n",
    "        cache_path = os.path.join(self.cache_dir, f\"{base}_seg{seg_idx}.npz\")\n",
    "\n",
    "        if os.path.exists(cache_path):\n",
    "            data = np.load(cache_path)\n",
    "            spec, mfcc, chroma = data['spec'], data['mfcc'], data['chroma']\n",
    "        else:\n",
    "            y, _ = librosa.load(os.path.join(self.audio_dir, fname), sr=SR)\n",
    "            \n",
    "            # Segment the full audio and select the correct one\n",
    "            segs = segment_audio(y, SR, WIN_SEC, HOP_SEC)\n",
    "            seg = segs[seg_idx]\n",
    "\n",
    "            spec   = make_spectrogram(seg, SR)\n",
    "            mfcc   = make_mfcc(seg, SR)\n",
    "            chroma = make_chroma(seg, SR)\n",
    "            np.savez(cache_path, spec=spec, mfcc=mfcc, chroma=chroma)\n",
    "\n",
    "        # Stack into 3-channel float tensor\n",
    "        img = np.stack([spec, mfcc, chroma], axis=0).astype(np.float32) / 255.0\n",
    "        if self.transform:\n",
    "            img = self.transform(torch.from_numpy(img))\n",
    "        \n",
    "        return img, label\n",
    "\n",
    "# Transforms\n",
    "transform = transforms.Normalize(mean=[0.5]*3, std=[0.2]*3)\n",
    "\n",
    "# Create Datasets using the new segment manifests\n",
    "train_ds = ICBHIDataset(df_train, AUDIO_DIR, CACHE_DIR, transform)\n",
    "test_ds  = ICBHIDataset(df_test,  AUDIO_DIR, CACHE_DIR, transform)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2) # num_workers adjusted for broader compatibility\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b18f8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /home/nhat-minh/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:04<00:00, 10.3MB/s]\n"
     ]
    }
   ],
   "source": [
    "class TripleStreamNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.streams = nn.ModuleList()\n",
    "        for _ in range(3):\n",
    "            m = models.resnet18(weights=models.ResNet18_Weights.DEFAULT) # Updated to new torchvision API\n",
    "            m.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "            m.fc    = nn.Identity()\n",
    "            self.streams.append(m)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(3 * 512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(256, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = []\n",
    "        for i in range(3):\n",
    "            xi = x[:, i:i+1, :, :]\n",
    "            fi = self.streams[i](xi)\n",
    "            feats.append(fi)\n",
    "        f = torch.cat(feats, dim=1)\n",
    "        return self.classifier(f)\n",
    "\n",
    "model = TripleStreamNet(num_classes=len(labels)).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58b72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for imgs, lbls in tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\"):\n",
    "        imgs, lbls = imgs.to(DEVICE), lbls.to(DEVICE)\n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, lbls)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"  → Train loss: {total_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682e5362",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_probs, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for imgs, lbls in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        logits = model(imgs)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_labels.append(lbls.numpy())\n",
    "# \n",
    "probs = np.vstack(all_probs)\n",
    "labels = np.hstack(all_labels)\n",
    "preds  = np.argmax(probs, axis=1)\n",
    "\n",
    "acc = (preds == labels).mean()\n",
    "print(f\"\\nTest Accuracy (per segment): {acc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
