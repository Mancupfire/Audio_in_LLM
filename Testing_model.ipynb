{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs Available: []\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torchmetrics import AUROC\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    LlamaConfig, \n",
    "    LlamaModel, \n",
    "    LlamaTokenizer, \n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoModel\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig, \n",
    "    IA3Config, \n",
    "    get_peft_model, \n",
    "    TaskType\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Các target modules cho PEFT\n",
    "OPERA_CT_TARGET_MODULES = [\"qkv\", \"proj\"]\n",
    "OPERA_CE_TARGET_MODULES = [\"conv\", \"fc\", \"linear\"]\n",
    "target_module_dict = {\n",
    "    \"operaCT\": OPERA_CT_TARGET_MODULES, \n",
    "    \"operaCE\": OPERA_CE_TARGET_MODULES\n",
    "}\n",
    "\n",
    "LLM_TARGET_MODULES = [\"q_proj\", \"v_proj\"]\n",
    "LLM_TARGET_MODULES_ALLPROJ = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OperaCTEncoder(nn.Module):\n",
    "    def __init__(self, in_dim=256, out_dim=256):\n",
    "        super().__init__()\n",
    "        # Đặt tên module như \"qkv\" và \"proj\" để PEFT có thể nhận diện\n",
    "        self.qkv = nn.Conv1d(in_dim, out_dim, kernel_size=3, padding=1)\n",
    "        self.proj = nn.Linear(out_dim, out_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.qkv(x)   # (B, out_dim, time)\n",
    "        x = F.relu(x)\n",
    "        # Global average pool theo chiều time\n",
    "        x = x.mean(dim=-1)  # (B, out_dim)\n",
    "\n",
    "        # Linear projection\n",
    "        x = self.proj(x)    # (B, out_dim)\n",
    "        return x\n",
    "\n",
    "    def forward_window(self, x):\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, nf, out_dim, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, out_dim)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x, no_fc=False):\n",
    "        x = self.flatten(x)\n",
    "        if no_fc:\n",
    "            return x\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RespLLM(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(RespLLM, self).__init__()\n",
    "\n",
    "        # Lưu các tham số từ configs\n",
    "        self.n_cls = configs.n_cls\n",
    "        self.d_ff = configs.d_ff\n",
    "        self.d_llm = configs.llm_dim\n",
    "        self.audio_peft = configs.audio_peft\n",
    "        self.d_audio = configs.enc_dim\n",
    "        self.patch_nums = configs.patch_nums\n",
    "        self.head_nf = self.d_ff * self.patch_nums\n",
    "\n",
    "        self.llm_peft = configs.llm_peft\n",
    "        self.llm_lora_rank = configs.llm_lora_rank\n",
    "        self.llm_lora_alpha = configs.llm_lora_alpha\n",
    "        self.llm_lora_dropout = configs.llm_lora_dropout\n",
    "\n",
    "        self.use_audio = configs.use_audio\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        self.validation_step_outputs = []\n",
    "        self.test_step_outputs = []\n",
    "\n",
    "        # LLM model initialization \n",
    "        if configs.llm_model == 'llama3':\n",
    "            # Thay meta-llama/Llama-2-7b-hf cho hợp lệ\n",
    "            self.llama_config = LlamaConfig.from_pretrained('meta-llama/Llama-3.1-8B')\n",
    "            try:\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    'meta-llama/Llama-3.1-8B',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.llama_config,\n",
    "                )\n",
    "            except EnvironmentError:\n",
    "                print(\"Không tìm thấy model cục bộ, thử tải về ...\")\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    'meta-llama/Llama-3.1-8B',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.llama_config,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    'meta-llama/Llama-3.1-8B',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "            except EnvironmentError:\n",
    "                print(\"Không tìm thấy tokenizer cục bộ, thử tải về ...\")\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    'meta-llama/Llama-3.1-8B',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "\n",
    "        # Khởi tạo Audio Encoder và bọc PEFT \n",
    "        # (Ví dụ: OperaCTEncoder -> LoRA)\n",
    "        self.base_audio_encoder = OperaCTEncoder(\n",
    "            in_dim=self.d_audio, \n",
    "            out_dim=self.d_audio\n",
    "        )\n",
    "\n",
    "        if self.audio_peft == \"lora\":\n",
    "            peft_config = LoraConfig(\n",
    "                r=configs.audio_lora_rank,\n",
    "                lora_alpha=32,\n",
    "                lora_dropout=0.1,\n",
    "                target_modules=target_module_dict[configs.audio_encoder]\n",
    "            )\n",
    "        elif self.audio_peft == \"IA3\":\n",
    "            peft_config = IA3Config(\n",
    "                target_modules=target_module_dict[configs.audio_encoder],\n",
    "                feedforward_modules=['proj']\n",
    "            )\n",
    "        else:\n",
    "            raise NotImplementedError(\"Audio fine-tuning mode undefined\")\n",
    "\n",
    "        # Bọc audio_encoder với PEFT\n",
    "        self.audio_encoder = get_peft_model(self.base_audio_encoder, peft_config)\n",
    "        self.audio_encoder.print_trainable_parameters()\n",
    "\n",
    "        # Aligner module (chuyển từ d_audio sang d_llm) \n",
    "        if configs.aligner == \"projection\":\n",
    "            self.aligner = nn.Linear(self.d_audio, self.d_llm)\n",
    "        else:\n",
    "            raise NotImplementedError(\"Aligner module undefined\")\n",
    "\n",
    "        # Output projection head \n",
    "        self.head_dropout = configs.head_dropout\n",
    "        self.output_projection = FlattenHead(self.head_nf, self.n_cls, head_dropout=self.head_dropout)\n",
    "\n",
    "        # In số lượng tham số trainable\n",
    "        self.print_trainable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinitialize_clf(self, n_cls):\n",
    "        self.output_projection = FlattenHead(self.head_nf, n_cls, head_dropout=self.head_dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable(self):\n",
    "    trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "    print(\"total trainable parameters:\", trainable_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_trainable(self):\n",
    "        # Nếu muốn frozen LLM (chẳng hạn)\n",
    "        if self.llm_peft == \"frozen\":\n",
    "            for param in self.llm_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        # Ngược lại nếu muốn LoRA cho LLM, có thể định nghĩa:\n",
    "        # (Ví dụ)\n",
    "        # if self.llm_peft == \"lora\":\n",
    "        #     # Gọi get_peft_model(self.llm_model, some_lora_config)\n",
    "        #     pass\n",
    "\n",
    "        # Audio encoder\n",
    "        if self.audio_peft == \"frozen\":\n",
    "            for param in self.audio_encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "        elif self.audio_peft == \"full\":\n",
    "            for param in self.audio_encoder.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        # Aligner\n",
    "        for param in self.aligner.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        # Output projection\n",
    "        for param in self.output_projection.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.print_trainable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, x_spectrogram, x_prompt, x_context, no_fc=False):\n",
    "        # Encode audio \n",
    "        if self.patch_nums == 1:\n",
    "            x_enc = self.audio_encoder(x_spectrogram)  # (B, d_audio)\n",
    "            enc_out = self.aligner(x_enc)              # (B, d_llm)\n",
    "            enc_out = enc_out.unsqueeze(dim=1)         # (B, 1, d_llm)\n",
    "        elif self.patch_nums == 64:\n",
    "            x_enc = self.audio_encoder.forward_window(x_spectrogram)\n",
    "            enc_out = self.aligner(x_enc)  # Giả sử (B, 64, d_llm) \n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        \n",
    "        # Tokenize prompt/context rồi lấy embeddings \n",
    "        prompt = self.tokenizer(\n",
    "            x_prompt, return_tensors=\"pt\", \n",
    "            padding=True, truncation=True, max_length=2048\n",
    "        ).input_ids.to(x_enc.device)\n",
    "\n",
    "        context = self.tokenizer(\n",
    "            x_context, return_tensors=\"pt\", \n",
    "            padding=True, truncation=True, max_length=2048\n",
    "        ).input_ids.to(x_enc.device)\n",
    "\n",
    "        prompt_embeddings = self.llm_model.get_input_embeddings()(prompt)     # (B, prompt_len, d_llm)\n",
    "        context_embeddings = self.llm_model.get_input_embeddings()(context)   # (B, context_len, d_llm)\n",
    "\n",
    "        # Ghép embeddings (text + audio) \n",
    "        if self.use_audio:\n",
    "            llama_enc_out = torch.cat([prompt_embeddings, context_embeddings, enc_out], dim=1)\n",
    "        else:\n",
    "            llama_enc_out = torch.cat([prompt_embeddings, context_embeddings], dim=1)\n",
    "\n",
    "        # Cho qua LLM \n",
    "        dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state\n",
    "        # dec_out shape: (B, seq_len, d_llm)\n",
    "\n",
    "        # Giả sử ta chỉ giữ lại d_ff đầu (nếu d_llm >= d_ff)\n",
    "        dec_out = dec_out[:, :, :self.d_ff]  # (B, seq_len, d_ff)\n",
    "        dec_out = dec_out.permute(0, 2, 1).contiguous()  # (B, d_ff, seq_len)\n",
    "\n",
    "        # Lấy phần cuối, hoặc patch cuối, rồi cho qua head \n",
    "        dec_out = dec_out[:, :, -self.patch_nums:]  # (B, d_ff, patch_nums)\n",
    "        logits = self.output_projection(dec_out, no_fc=no_fc)  # (B, n_cls)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    n_cls = 10\n",
    "    d_ff = 512\n",
    "    llm_dim = 1024\n",
    "    audio_peft = \"lora\"\n",
    "    enc_dim = 256\n",
    "    patch_nums = 1\n",
    "    llm_peft = \"frozen\"     # hoặc \"lora\" nếu muốn LoRA cho LLM\n",
    "    llm_lora_rank = 8\n",
    "    llm_lora_alpha = 32\n",
    "    llm_lora_dropout = 0.1\n",
    "    use_audio = True\n",
    "    llm_model = \"llama3\"    # giả sử 'llama3' tương ứng Llama3.3 70B HF\n",
    "    audio_lora_rank = 4\n",
    "    audio_encoder = \"operaCT\"\n",
    "    aligner = \"projection\"\n",
    "    head_dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaModel\n",
    "\n",
    "model = LlamaModel.from_pretrained(\n",
    "  \"meta-llama/Llama-3.1-8B\",\n",
    "  use_auth_token=\"[Hugging Face API Here]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo instance và chạy thử forward\n",
    "if __name__ == \"__main__\":\n",
    "    configs = Config()\n",
    "    model = RespLLM(configs)\n",
    "\n",
    "    # Tạo input ví dụ\n",
    "    x_spectrogram = torch.randn(1, 256, 64)  # (batch=1, in_dim=256, time=64)\n",
    "    x_prompt = \"Đây là một prompt thử nghiệm.\"\n",
    "    x_context = \"Đây là phần context thử nghiệm.\"\n",
    "\n",
    "    # Forward\n",
    "    output = model.forward(x_spectrogram, x_prompt, x_context)\n",
    "    print(\"Output shape:\", output.shape)\n",
    "    print(\"Output:\", output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
